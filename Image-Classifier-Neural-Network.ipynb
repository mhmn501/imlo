{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from PyTorch\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Import necessary modules for Neural Network\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Import necessary modules for plotting\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Convolution Neural Network\n",
    "# Simple CNN architecture with two convolutional layers followed by max pooling, two fully connected layers, and a dropout layer for regularization.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_out_ch=[32, 64, 128, 256], img_w=100, img_h=100, dropout=0.5, num_classes=102):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.layer1 = nn.Sequential( #This is technically not a type of layer but it helps in combining different operations that are part of the same step\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=num_out_ch[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[0]), # This applies batch normalization to the output from the convolutional layer\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[0], out_channels=num_out_ch[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # Max pooling layer: down-sample an image by applying max filer to subregion\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[0], out_channels=num_out_ch[1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[1], out_channels=num_out_ch[1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[1], out_channels=num_out_ch[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[2], out_channels=num_out_ch[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[2], out_channels=num_out_ch[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[2], out_channels=num_out_ch[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[3]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[3], out_channels=num_out_ch[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[3]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[3], out_channels=num_out_ch[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[3], out_channels=num_out_ch[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[3]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[3], out_channels=num_out_ch[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[3]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_out_ch[3], out_channels=num_out_ch[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_out_ch[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout), # Dropout layer to prevent overfitting\n",
    "            nn.Linear(7*7*num_out_ch[3], 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    # Defines the forward pass of the network, where input data x is passed through each layer sequentially.\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Parameters, Device, Model, Optimizer, and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_OUT_CH = [4, 8, 16, 32]\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8\n",
    "NUM_EPOCHS = 1000  # Number of training epochs\n",
    "LR = 0.0001\n",
    "WEIGHT_DECAY = 0.001\n",
    "MOMENTUM = 0.9\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = CNN(num_channels=3, num_out_ch=NUM_OUT_CH, dropout=DROPOUT, num_classes=102).to(device)  # 102 classes for Flowers102 dataset\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprosessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Randomly crop and resize the image\n",
    "    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(5),       # Randomly rotate the image by up to 10 degrees\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2),  # Randomly adjust brightness, contrast, saturation\n",
    "    transforms.ToTensor(),               # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),              # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),          # Crop the center of the image to 224x224\n",
    "    transforms.ToTensor(),               # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Define dataset root directory\n",
    "data_dir = 'dataset_flower102/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the dataset during data loading\n",
    "train_dataset = datasets.Flowers102(root=data_dir, split='train', transform=train_transform, download=True)\n",
    "valid_dataset = datasets.Flowers102(root=data_dir, split='val', transform=val_transform, download=True)\n",
    "# test_dataset = datasets.Flowers102(root=data_dir, split='test', transform=data_transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_val_loss:  2.327421027538823\n",
      "Epoch 609/1000, Training Loss: 3.2856, Training Accuracy: 28.82%\n",
      "Epoch 609/1000, Validation Loss: 2.3767, Validation Accuracy: 45.20%\n",
      "Epoch 610/1000, Training Loss: 3.3635, Training Accuracy: 28.82%\n",
      "Epoch 610/1000, Validation Loss: 2.4208, Validation Accuracy: 45.10%\n",
      "Epoch 611/1000, Training Loss: 3.4175, Training Accuracy: 27.94%\n",
      "Epoch 611/1000, Validation Loss: 2.4891, Validation Accuracy: 43.14%\n",
      "Epoch 612/1000, Training Loss: 3.5214, Training Accuracy: 25.29%\n",
      "Epoch 612/1000, Validation Loss: 2.4470, Validation Accuracy: 44.31%\n",
      "Epoch 613/1000, Training Loss: 3.4815, Training Accuracy: 28.04%\n",
      "Epoch 613/1000, Validation Loss: 2.4514, Validation Accuracy: 44.12%\n",
      "Epoch 614/1000, Training Loss: 3.3772, Training Accuracy: 29.71%\n",
      "Epoch 614/1000, Validation Loss: 2.4285, Validation Accuracy: 44.90%\n",
      "Epoch 615/1000, Training Loss: 3.4906, Training Accuracy: 28.14%\n",
      "Epoch 615/1000, Validation Loss: 2.4399, Validation Accuracy: 45.20%\n",
      "Epoch 616/1000, Training Loss: 3.3873, Training Accuracy: 28.33%\n",
      "Epoch 616/1000, Validation Loss: 2.4280, Validation Accuracy: 45.00%\n",
      "Epoch 617/1000, Training Loss: 3.2629, Training Accuracy: 28.43%\n",
      "Epoch 617/1000, Validation Loss: 2.4453, Validation Accuracy: 44.61%\n",
      "Epoch 618/1000, Training Loss: 3.4327, Training Accuracy: 29.51%\n",
      "Epoch 618/1000, Validation Loss: 2.4351, Validation Accuracy: 44.51%\n",
      "Epoch 619/1000, Training Loss: 3.4952, Training Accuracy: 26.18%\n",
      "Epoch 619/1000, Validation Loss: 2.4335, Validation Accuracy: 44.41%\n",
      "Epoch 620/1000, Training Loss: 3.4405, Training Accuracy: 28.14%\n",
      "Epoch 620/1000, Validation Loss: 2.4439, Validation Accuracy: 44.12%\n",
      "Epoch 621/1000, Training Loss: 3.4834, Training Accuracy: 26.96%\n",
      "Epoch 621/1000, Validation Loss: 2.4655, Validation Accuracy: 44.02%\n",
      "Epoch 622/1000, Training Loss: 3.3749, Training Accuracy: 28.43%\n",
      "Epoch 622/1000, Validation Loss: 2.4950, Validation Accuracy: 42.45%\n",
      "Epoch 623/1000, Training Loss: 3.5728, Training Accuracy: 27.65%\n",
      "Epoch 623/1000, Validation Loss: 2.4549, Validation Accuracy: 44.31%\n",
      "Epoch 624/1000, Training Loss: 3.2805, Training Accuracy: 29.41%\n",
      "Epoch 624/1000, Validation Loss: 2.4366, Validation Accuracy: 44.71%\n",
      "Epoch 625/1000, Training Loss: 3.3028, Training Accuracy: 30.20%\n",
      "Epoch 625/1000, Validation Loss: 2.4254, Validation Accuracy: 44.51%\n",
      "Epoch 626/1000, Training Loss: 3.3268, Training Accuracy: 27.94%\n",
      "Epoch 626/1000, Validation Loss: 2.4453, Validation Accuracy: 43.63%\n",
      "Epoch 627/1000, Training Loss: 3.4072, Training Accuracy: 29.80%\n",
      "Epoch 627/1000, Validation Loss: 2.4562, Validation Accuracy: 43.63%\n",
      "Epoch 628/1000, Training Loss: 3.3683, Training Accuracy: 29.31%\n",
      "Epoch 628/1000, Validation Loss: 2.4409, Validation Accuracy: 44.31%\n",
      "Epoch 629/1000, Training Loss: 3.3798, Training Accuracy: 29.61%\n",
      "Epoch 629/1000, Validation Loss: 2.4588, Validation Accuracy: 44.61%\n",
      "Epoch 630/1000, Training Loss: 3.3137, Training Accuracy: 30.39%\n",
      "Epoch 630/1000, Validation Loss: 2.4395, Validation Accuracy: 45.00%\n",
      "Epoch 631/1000, Training Loss: 3.3718, Training Accuracy: 28.43%\n",
      "Epoch 631/1000, Validation Loss: 2.4551, Validation Accuracy: 43.92%\n",
      "Epoch 632/1000, Training Loss: 3.3008, Training Accuracy: 29.80%\n",
      "Epoch 632/1000, Validation Loss: 2.4363, Validation Accuracy: 45.20%\n",
      "Epoch 633/1000, Training Loss: 3.4370, Training Accuracy: 30.00%\n",
      "Epoch 633/1000, Validation Loss: 2.4438, Validation Accuracy: 44.90%\n",
      "Epoch 634/1000, Training Loss: 3.4422, Training Accuracy: 28.04%\n",
      "Epoch 634/1000, Validation Loss: 2.4368, Validation Accuracy: 44.41%\n",
      "Epoch 635/1000, Training Loss: 3.3891, Training Accuracy: 28.24%\n",
      "Epoch 635/1000, Validation Loss: 2.4322, Validation Accuracy: 44.31%\n",
      "Epoch 636/1000, Training Loss: 3.4257, Training Accuracy: 28.92%\n",
      "Epoch 636/1000, Validation Loss: 2.4316, Validation Accuracy: 45.10%\n",
      "Epoch 637/1000, Training Loss: 3.2611, Training Accuracy: 31.86%\n",
      "Epoch 637/1000, Validation Loss: 2.4251, Validation Accuracy: 44.90%\n",
      "Epoch 638/1000, Training Loss: 3.3813, Training Accuracy: 30.00%\n",
      "Epoch 638/1000, Validation Loss: 2.4219, Validation Accuracy: 45.20%\n",
      "Epoch 639/1000, Training Loss: 3.5118, Training Accuracy: 26.27%\n",
      "Epoch 639/1000, Validation Loss: 2.4371, Validation Accuracy: 44.90%\n",
      "Epoch 640/1000, Training Loss: 3.4363, Training Accuracy: 26.67%\n",
      "Epoch 640/1000, Validation Loss: 2.4395, Validation Accuracy: 44.41%\n",
      "Epoch 641/1000, Training Loss: 3.4713, Training Accuracy: 28.92%\n",
      "Epoch 641/1000, Validation Loss: 2.4491, Validation Accuracy: 45.29%\n",
      "Epoch 642/1000, Training Loss: 3.3709, Training Accuracy: 29.80%\n",
      "Epoch 642/1000, Validation Loss: 2.4517, Validation Accuracy: 44.61%\n",
      "Epoch 643/1000, Training Loss: 3.4648, Training Accuracy: 28.24%\n",
      "Epoch 643/1000, Validation Loss: 2.4618, Validation Accuracy: 44.61%\n",
      "Epoch 644/1000, Training Loss: 3.4911, Training Accuracy: 28.04%\n",
      "Epoch 644/1000, Validation Loss: 2.4688, Validation Accuracy: 43.63%\n",
      "Epoch 645/1000, Training Loss: 3.4928, Training Accuracy: 27.45%\n",
      "Epoch 645/1000, Validation Loss: 2.4510, Validation Accuracy: 44.71%\n",
      "Epoch 646/1000, Training Loss: 3.3637, Training Accuracy: 28.73%\n",
      "Epoch 646/1000, Validation Loss: 2.4683, Validation Accuracy: 45.00%\n",
      "Epoch 647/1000, Training Loss: 3.5321, Training Accuracy: 28.82%\n",
      "Epoch 647/1000, Validation Loss: 2.4536, Validation Accuracy: 44.02%\n",
      "Epoch 648/1000, Training Loss: 3.4116, Training Accuracy: 29.22%\n",
      "Epoch 648/1000, Validation Loss: 2.4915, Validation Accuracy: 43.33%\n",
      "Epoch 649/1000, Training Loss: 3.3498, Training Accuracy: 28.92%\n",
      "Epoch 649/1000, Validation Loss: 2.4529, Validation Accuracy: 43.73%\n",
      "Epoch 650/1000, Training Loss: 3.4151, Training Accuracy: 28.63%\n",
      "Epoch 650/1000, Validation Loss: 2.4212, Validation Accuracy: 44.41%\n",
      "Epoch 651/1000, Training Loss: 3.4993, Training Accuracy: 27.45%\n",
      "Epoch 651/1000, Validation Loss: 2.4377, Validation Accuracy: 44.71%\n",
      "Epoch 652/1000, Training Loss: 3.4551, Training Accuracy: 29.12%\n",
      "Epoch 652/1000, Validation Loss: 2.4279, Validation Accuracy: 45.10%\n",
      "Epoch 653/1000, Training Loss: 3.4940, Training Accuracy: 28.53%\n",
      "Epoch 653/1000, Validation Loss: 2.4346, Validation Accuracy: 43.82%\n",
      "Epoch 654/1000, Training Loss: 3.3334, Training Accuracy: 29.12%\n",
      "Epoch 654/1000, Validation Loss: 2.4428, Validation Accuracy: 43.92%\n",
      "Epoch 655/1000, Training Loss: 3.3196, Training Accuracy: 29.61%\n",
      "Epoch 655/1000, Validation Loss: 2.4210, Validation Accuracy: 44.61%\n",
      "Epoch 656/1000, Training Loss: 3.4066, Training Accuracy: 29.51%\n",
      "Epoch 656/1000, Validation Loss: 2.4393, Validation Accuracy: 44.22%\n",
      "Epoch 657/1000, Training Loss: 3.3125, Training Accuracy: 29.31%\n",
      "Epoch 657/1000, Validation Loss: 2.4598, Validation Accuracy: 44.02%\n",
      "Epoch 658/1000, Training Loss: 3.3088, Training Accuracy: 30.59%\n",
      "Epoch 658/1000, Validation Loss: 2.4561, Validation Accuracy: 44.22%\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "# Optionally resume from the best trained model along with training and validation loss values\n",
    "resume_from_best_checkpoint = True\n",
    "if resume_from_best_checkpoint:\n",
    "    checkpoint = torch.load('best_trained_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state']) # Set the model state and loss values before resuming training (Need to comment out the train_loss and val_loss)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    train_loss_history = checkpoint['train_loss']\n",
    "    val_loss_history = checkpoint['val_loss']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss with a large value (float('inf'))\n",
    "    \n",
    "\n",
    "print(\"best_val_loss: \", best_val_loss)\n",
    "\n",
    "patience = 50  # Number of epochs to wait before stopping if validation loss doesn't improve\n",
    "\n",
    "for epoch in range(start_epoch+1, NUM_EPOCHS):\n",
    "    \n",
    "    # Train the model\n",
    "    model.train() # Set the model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device) # Move images and labels to GPU\n",
    "\n",
    "        outputs = model.forward(images) # Forward pass\n",
    "        loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Optimize\n",
    "\n",
    "        running_train_loss += loss.item() * images.size(0) #  scalar value of the loss tensor for the current batch * the batch size to account for the loss per sample in the batch\n",
    "        _, predicted = outputs.max(1) # Returns a tuple containing the maximum value along the specified dimension (class probabilities for each sample in the batch) and index of the max value\n",
    "        total += labels.size(0) # Accumulates the total number of sample seen during training\n",
    "        correct += predicted.eq(labels).sum().item() # Accumulates the total number of correct predictions over all batches.\n",
    "    \n",
    "    # Calculate training loss and accuracy\n",
    "    train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy = 100.0 * correct / total\n",
    "\n",
    "    # Print training loss and accuracy\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device) # Move images and labels to GPU\n",
    "\n",
    "            outputs = model.forward(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Calculate the loss\n",
    "\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss = running_val_loss / len(valid_loader.dataset)\n",
    "    val_loss_history.append(train_loss)\n",
    "    val_accuracy = 100.0 * correct / total\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print validation loss and accuracy\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"Creating new checkpoint for best model...\")\n",
    "        best_val_loss = val_loss\n",
    "        patience = 50  # Reset patience if validation loss improves\n",
    "\n",
    "        # Save the best trained model along with training and validation loss values\n",
    "        torch.save({\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss_history,\n",
    "            'val_loss': val_loss_history, \n",
    "            'best_val_loss': best_val_loss\n",
    "        }, 'best_trained_model.pth')\n",
    "\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "# save the completed model training\n",
    "finish_model_state = model.state_dict()\n",
    "torch.save({\n",
    "    'model_state': finish_model_state,\n",
    "    'optimizer_state': optimizer.state_dict(),\n",
    "    'scheduler_state': scheduler.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'train_loss': train_loss_history,\n",
    "    'val_loss': val_loss_history,\n",
    "    'best_val_loss': best_val_loss\n",
    "}, 'finish_trained_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
