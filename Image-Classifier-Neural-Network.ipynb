{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from PyTorch\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Import necessary modules for Neural Network\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Convolution Neural Network\n",
    "# Simple CNN architecture with two convolutional layers followed by max pooling, two fully connected layers, and a dropout layer for regularization.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_out_ch=[8, 16], img_w=100, img_h=100, dropout=0.5, num_classes=102):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=num_out_ch[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_out_ch[0], out_channels=num_out_ch[1], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max pooling layer: down-sample an image by applying max filer to subregion\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(in_features = 16 * 56 * 56, out_features=num_classes)\n",
    "        # self.fc1 = nn.Linear(32 * 56 * 56, 512)  # Adjust input size based on image dimensions\n",
    "        # self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    \n",
    "    # Defines the forward pass of the network, where input data x is passed through each layer sequentially.\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convolutional layers with ReLU activation and max pooling\n",
    "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
    "        \n",
    "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten the output from convolutional layers\n",
    "        # x = x.view(-1, 32 * 56 * 56)  # Adjust size based on image dimensions\n",
    "        x = x.view(x.size(0), -1)  # Adjust size based on image dimensions\n",
    "\n",
    "        # Fully connected layers with dropout\n",
    "        x = self.fc(x.reshape(x.shape[0], -1))\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.dropout(nn.ReLU()(self.fc1(x)))\n",
    "        # x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Parameters, Device, Model, Optimizer, and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_OUT_CH = [8, 16]\n",
    "IMAGE_W = 200\n",
    "IMAGE_H = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8\n",
    "NUM_EPOCHS = 100  # Number of training epochs\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = CNN(num_channels=3, num_out_ch=NUM_OUT_CH, img_w=IMAGE_W, img_h=IMAGE_H, dropout=DROPOUT, num_classes=102).to(device)  # 102 classes for Flowers102 dataset\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprosessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Randomly crop and resize the image\n",
    "    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(10),       # Randomly rotate the image by up to 10 degrees\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2),  # Randomly adjust brightness, contrast, and saturation\n",
    "    transforms.ToTensor(),               # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),              # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),          # Crop the center of the image to 224x224\n",
    "    transforms.ToTensor(),               # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Define dataset root directory\n",
    "data_dir = 'dataset_flower102/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the dataset during data loading\n",
    "train_dataset = datasets.Flowers102(root=data_dir, split='train', transform=train_transform, download=True)\n",
    "valid_dataset = datasets.Flowers102(root=data_dir, split='val', transform=val_transform, download=True)\n",
    "# test_dataset = datasets.Flowers102(root=data_dir, split='test', transform=data_transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 6.2501, Training Accuracy: 1.47%\n",
      "Epoch 1/100, Validation Loss: 4.5504, Validation Accuracy: 4.02%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 2/100, Training Loss: 4.4031, Training Accuracy: 4.12%\n",
      "Epoch 2/100, Validation Loss: 4.1405, Validation Accuracy: 6.67%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 3/100, Training Loss: 4.1131, Training Accuracy: 6.76%\n",
      "Epoch 3/100, Validation Loss: 3.9450, Validation Accuracy: 8.82%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 4/100, Training Loss: 3.9317, Training Accuracy: 7.94%\n",
      "Epoch 4/100, Validation Loss: 3.7902, Validation Accuracy: 11.08%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 5/100, Training Loss: 3.7708, Training Accuracy: 11.47%\n",
      "Epoch 5/100, Validation Loss: 3.6297, Validation Accuracy: 13.43%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 6/100, Training Loss: 3.6379, Training Accuracy: 15.00%\n",
      "Epoch 6/100, Validation Loss: 3.4777, Validation Accuracy: 16.96%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 7/100, Training Loss: 3.4598, Training Accuracy: 19.02%\n",
      "Epoch 7/100, Validation Loss: 3.3702, Validation Accuracy: 19.41%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 8/100, Training Loss: 3.3215, Training Accuracy: 19.90%\n",
      "Epoch 8/100, Validation Loss: 3.2845, Validation Accuracy: 19.61%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 9/100, Training Loss: 3.1657, Training Accuracy: 23.14%\n",
      "Epoch 9/100, Validation Loss: 3.1725, Validation Accuracy: 22.65%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 10/100, Training Loss: 3.1329, Training Accuracy: 22.65%\n",
      "Epoch 10/100, Validation Loss: 3.1053, Validation Accuracy: 23.43%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 11/100, Training Loss: 2.9830, Training Accuracy: 26.18%\n",
      "Epoch 11/100, Validation Loss: 3.0689, Validation Accuracy: 26.86%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 12/100, Training Loss: 2.9302, Training Accuracy: 27.55%\n",
      "Epoch 12/100, Validation Loss: 3.0039, Validation Accuracy: 26.86%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 13/100, Training Loss: 2.8665, Training Accuracy: 30.20%\n",
      "Epoch 13/100, Validation Loss: 3.0182, Validation Accuracy: 27.84%\n",
      "Epoch 14/100, Training Loss: 2.8730, Training Accuracy: 28.92%\n",
      "Epoch 14/100, Validation Loss: 2.9768, Validation Accuracy: 28.33%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 15/100, Training Loss: 2.8053, Training Accuracy: 30.20%\n",
      "Epoch 15/100, Validation Loss: 3.0350, Validation Accuracy: 27.94%\n",
      "Epoch 16/100, Training Loss: 2.7487, Training Accuracy: 30.88%\n",
      "Epoch 16/100, Validation Loss: 2.9057, Validation Accuracy: 30.98%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 17/100, Training Loss: 2.6423, Training Accuracy: 32.06%\n",
      "Epoch 17/100, Validation Loss: 2.9489, Validation Accuracy: 30.49%\n",
      "Epoch 18/100, Training Loss: 2.6002, Training Accuracy: 33.92%\n",
      "Epoch 18/100, Validation Loss: 2.9419, Validation Accuracy: 31.08%\n",
      "Epoch 19/100, Training Loss: 2.5255, Training Accuracy: 35.39%\n",
      "Epoch 19/100, Validation Loss: 2.8597, Validation Accuracy: 32.06%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 20/100, Training Loss: 2.5451, Training Accuracy: 37.06%\n",
      "Epoch 20/100, Validation Loss: 2.8518, Validation Accuracy: 33.73%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 21/100, Training Loss: 2.4656, Training Accuracy: 37.55%\n",
      "Epoch 21/100, Validation Loss: 2.8139, Validation Accuracy: 34.41%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 22/100, Training Loss: 2.4333, Training Accuracy: 37.55%\n",
      "Epoch 22/100, Validation Loss: 2.8275, Validation Accuracy: 34.12%\n",
      "Epoch 23/100, Training Loss: 2.4142, Training Accuracy: 37.84%\n",
      "Epoch 23/100, Validation Loss: 2.8332, Validation Accuracy: 35.10%\n",
      "Epoch 24/100, Training Loss: 2.3417, Training Accuracy: 40.29%\n",
      "Epoch 24/100, Validation Loss: 2.8523, Validation Accuracy: 34.90%\n",
      "Epoch 25/100, Training Loss: 2.4007, Training Accuracy: 39.80%\n",
      "Epoch 25/100, Validation Loss: 2.7826, Validation Accuracy: 34.51%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 26/100, Training Loss: 2.3702, Training Accuracy: 41.08%\n",
      "Epoch 26/100, Validation Loss: 2.7847, Validation Accuracy: 34.02%\n",
      "Epoch 27/100, Training Loss: 2.4223, Training Accuracy: 40.00%\n",
      "Epoch 27/100, Validation Loss: 2.7278, Validation Accuracy: 34.41%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 28/100, Training Loss: 2.2637, Training Accuracy: 40.49%\n",
      "Epoch 28/100, Validation Loss: 2.7705, Validation Accuracy: 35.78%\n",
      "Epoch 29/100, Training Loss: 2.2870, Training Accuracy: 41.76%\n",
      "Epoch 29/100, Validation Loss: 2.8204, Validation Accuracy: 33.82%\n",
      "Epoch 30/100, Training Loss: 2.3348, Training Accuracy: 41.96%\n",
      "Epoch 30/100, Validation Loss: 2.7144, Validation Accuracy: 35.88%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 31/100, Training Loss: 2.2790, Training Accuracy: 42.16%\n",
      "Epoch 31/100, Validation Loss: 2.7442, Validation Accuracy: 36.37%\n",
      "Epoch 32/100, Training Loss: 2.1626, Training Accuracy: 45.20%\n",
      "Epoch 32/100, Validation Loss: 2.7997, Validation Accuracy: 35.98%\n",
      "Epoch 33/100, Training Loss: 2.2964, Training Accuracy: 42.94%\n",
      "Epoch 33/100, Validation Loss: 2.8081, Validation Accuracy: 35.20%\n",
      "Epoch 34/100, Training Loss: 2.1424, Training Accuracy: 45.59%\n",
      "Epoch 34/100, Validation Loss: 2.8610, Validation Accuracy: 36.37%\n",
      "Epoch 35/100, Training Loss: 2.2075, Training Accuracy: 44.31%\n",
      "Epoch 35/100, Validation Loss: 2.8229, Validation Accuracy: 36.76%\n",
      "Epoch 36/100, Training Loss: 2.1746, Training Accuracy: 45.98%\n",
      "Epoch 36/100, Validation Loss: 2.7970, Validation Accuracy: 35.59%\n",
      "Epoch 37/100, Training Loss: 2.0532, Training Accuracy: 48.82%\n",
      "Epoch 37/100, Validation Loss: 2.8058, Validation Accuracy: 36.96%\n",
      "Epoch 38/100, Training Loss: 2.2298, Training Accuracy: 42.84%\n",
      "Epoch 38/100, Validation Loss: 2.8524, Validation Accuracy: 35.59%\n",
      "Epoch 39/100, Training Loss: 2.1179, Training Accuracy: 45.78%\n",
      "Epoch 39/100, Validation Loss: 2.7939, Validation Accuracy: 36.37%\n",
      "Epoch 40/100, Training Loss: 2.1219, Training Accuracy: 46.96%\n",
      "Epoch 40/100, Validation Loss: 2.7509, Validation Accuracy: 39.12%\n",
      "Epoch 41/100, Training Loss: 2.0417, Training Accuracy: 48.73%\n",
      "Epoch 41/100, Validation Loss: 2.7993, Validation Accuracy: 36.86%\n",
      "Epoch 42/100, Training Loss: 2.0103, Training Accuracy: 48.73%\n",
      "Epoch 42/100, Validation Loss: 2.8612, Validation Accuracy: 38.24%\n",
      "Epoch 43/100, Training Loss: 2.1324, Training Accuracy: 45.98%\n",
      "Epoch 43/100, Validation Loss: 2.7579, Validation Accuracy: 35.78%\n",
      "Epoch 44/100, Training Loss: 2.1359, Training Accuracy: 46.86%\n",
      "Epoch 44/100, Validation Loss: 2.8243, Validation Accuracy: 37.06%\n",
      "Epoch 45/100, Training Loss: 2.0338, Training Accuracy: 47.84%\n",
      "Epoch 45/100, Validation Loss: 2.7820, Validation Accuracy: 38.24%\n",
      "Epoch 46/100, Training Loss: 1.9844, Training Accuracy: 49.12%\n",
      "Epoch 46/100, Validation Loss: 2.8322, Validation Accuracy: 37.25%\n",
      "Epoch 47/100, Training Loss: 2.0274, Training Accuracy: 47.25%\n",
      "Epoch 47/100, Validation Loss: 2.7596, Validation Accuracy: 39.80%\n",
      "Epoch 48/100, Training Loss: 1.9788, Training Accuracy: 49.61%\n",
      "Epoch 48/100, Validation Loss: 2.7450, Validation Accuracy: 40.10%\n",
      "Epoch 49/100, Training Loss: 1.9602, Training Accuracy: 49.22%\n",
      "Epoch 49/100, Validation Loss: 2.7530, Validation Accuracy: 40.10%\n",
      "Epoch 50/100, Training Loss: 1.9295, Training Accuracy: 50.88%\n",
      "Epoch 50/100, Validation Loss: 2.7109, Validation Accuracy: 39.71%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 51/100, Training Loss: 1.9826, Training Accuracy: 49.12%\n",
      "Epoch 51/100, Validation Loss: 2.7947, Validation Accuracy: 38.73%\n",
      "Epoch 52/100, Training Loss: 1.8851, Training Accuracy: 50.10%\n",
      "Epoch 52/100, Validation Loss: 2.7561, Validation Accuracy: 39.41%\n",
      "Epoch 53/100, Training Loss: 1.9165, Training Accuracy: 53.82%\n",
      "Epoch 53/100, Validation Loss: 2.7906, Validation Accuracy: 37.55%\n",
      "Epoch 54/100, Training Loss: 2.0549, Training Accuracy: 47.94%\n",
      "Epoch 54/100, Validation Loss: 2.6883, Validation Accuracy: 39.71%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 55/100, Training Loss: 1.9590, Training Accuracy: 51.27%\n",
      "Epoch 55/100, Validation Loss: 2.8799, Validation Accuracy: 35.78%\n",
      "Epoch 56/100, Training Loss: 1.9145, Training Accuracy: 52.25%\n",
      "Epoch 56/100, Validation Loss: 2.6657, Validation Accuracy: 39.80%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 57/100, Training Loss: 1.9026, Training Accuracy: 50.49%\n",
      "Epoch 57/100, Validation Loss: 2.7745, Validation Accuracy: 38.53%\n",
      "Epoch 58/100, Training Loss: 1.8730, Training Accuracy: 51.18%\n",
      "Epoch 58/100, Validation Loss: 2.7170, Validation Accuracy: 39.31%\n",
      "Epoch 59/100, Training Loss: 1.8370, Training Accuracy: 52.65%\n",
      "Epoch 59/100, Validation Loss: 2.8053, Validation Accuracy: 39.22%\n",
      "Epoch 60/100, Training Loss: 1.9504, Training Accuracy: 50.00%\n",
      "Epoch 60/100, Validation Loss: 2.7971, Validation Accuracy: 38.43%\n",
      "Epoch 61/100, Training Loss: 1.7866, Training Accuracy: 54.61%\n",
      "Epoch 61/100, Validation Loss: 2.8337, Validation Accuracy: 38.63%\n",
      "Epoch 62/100, Training Loss: 1.8659, Training Accuracy: 50.78%\n",
      "Epoch 62/100, Validation Loss: 2.6646, Validation Accuracy: 40.20%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 63/100, Training Loss: 1.8597, Training Accuracy: 52.65%\n",
      "Epoch 63/100, Validation Loss: 2.7222, Validation Accuracy: 40.59%\n",
      "Epoch 64/100, Training Loss: 1.8396, Training Accuracy: 55.10%\n",
      "Epoch 64/100, Validation Loss: 2.6806, Validation Accuracy: 40.00%\n",
      "Epoch 65/100, Training Loss: 1.7913, Training Accuracy: 53.14%\n",
      "Epoch 65/100, Validation Loss: 2.6719, Validation Accuracy: 40.59%\n",
      "Epoch 66/100, Training Loss: 1.8172, Training Accuracy: 51.76%\n",
      "Epoch 66/100, Validation Loss: 2.7052, Validation Accuracy: 41.57%\n",
      "Epoch 67/100, Training Loss: 1.8223, Training Accuracy: 55.49%\n",
      "Epoch 67/100, Validation Loss: 2.6895, Validation Accuracy: 40.69%\n",
      "Epoch 68/100, Training Loss: 1.7835, Training Accuracy: 54.61%\n",
      "Epoch 68/100, Validation Loss: 2.6976, Validation Accuracy: 39.41%\n",
      "Epoch 69/100, Training Loss: 1.7504, Training Accuracy: 54.41%\n",
      "Epoch 69/100, Validation Loss: 2.7108, Validation Accuracy: 41.37%\n",
      "Epoch 70/100, Training Loss: 1.7051, Training Accuracy: 54.80%\n",
      "Epoch 70/100, Validation Loss: 2.7216, Validation Accuracy: 40.49%\n",
      "Epoch 71/100, Training Loss: 1.8005, Training Accuracy: 54.31%\n",
      "Epoch 71/100, Validation Loss: 2.7784, Validation Accuracy: 39.90%\n",
      "Epoch 72/100, Training Loss: 1.8057, Training Accuracy: 52.55%\n",
      "Epoch 72/100, Validation Loss: 2.7150, Validation Accuracy: 40.49%\n",
      "Epoch 73/100, Training Loss: 1.7826, Training Accuracy: 54.02%\n",
      "Epoch 73/100, Validation Loss: 2.6607, Validation Accuracy: 42.75%\n",
      "Creating new checkpoint for best model...\n",
      "Epoch 74/100, Training Loss: 1.7407, Training Accuracy: 55.00%\n",
      "Epoch 74/100, Validation Loss: 2.7081, Validation Accuracy: 41.47%\n",
      "Epoch 75/100, Training Loss: 1.8370, Training Accuracy: 53.92%\n",
      "Epoch 75/100, Validation Loss: 2.8259, Validation Accuracy: 40.20%\n",
      "Epoch 76/100, Training Loss: 1.6867, Training Accuracy: 55.20%\n",
      "Epoch 76/100, Validation Loss: 2.6978, Validation Accuracy: 42.25%\n",
      "Epoch 77/100, Training Loss: 1.7438, Training Accuracy: 54.71%\n",
      "Epoch 77/100, Validation Loss: 2.7409, Validation Accuracy: 41.27%\n",
      "Epoch 78/100, Training Loss: 1.9291, Training Accuracy: 49.80%\n",
      "Epoch 78/100, Validation Loss: 2.7352, Validation Accuracy: 40.78%\n",
      "Epoch 79/100, Training Loss: 1.7261, Training Accuracy: 55.10%\n",
      "Epoch 79/100, Validation Loss: 2.7550, Validation Accuracy: 41.08%\n",
      "Epoch 80/100, Training Loss: 1.7076, Training Accuracy: 56.86%\n",
      "Epoch 80/100, Validation Loss: 2.6901, Validation Accuracy: 40.59%\n",
      "Epoch 81/100, Training Loss: 1.7385, Training Accuracy: 54.80%\n",
      "Epoch 81/100, Validation Loss: 2.7240, Validation Accuracy: 41.37%\n",
      "Epoch 82/100, Training Loss: 1.7417, Training Accuracy: 56.37%\n",
      "Epoch 82/100, Validation Loss: 2.8023, Validation Accuracy: 40.98%\n",
      "Epoch 83/100, Training Loss: 1.7268, Training Accuracy: 55.78%\n",
      "Epoch 83/100, Validation Loss: 2.7579, Validation Accuracy: 41.08%\n",
      "Epoch 84/100, Training Loss: 1.7263, Training Accuracy: 55.10%\n",
      "Epoch 84/100, Validation Loss: 2.7295, Validation Accuracy: 40.20%\n",
      "Epoch 85/100, Training Loss: 1.6953, Training Accuracy: 56.86%\n",
      "Epoch 85/100, Validation Loss: 2.6723, Validation Accuracy: 40.29%\n",
      "Epoch 86/100, Training Loss: 1.7280, Training Accuracy: 57.16%\n",
      "Epoch 86/100, Validation Loss: 2.6963, Validation Accuracy: 42.06%\n",
      "Epoch 87/100, Training Loss: 1.5986, Training Accuracy: 57.55%\n",
      "Epoch 87/100, Validation Loss: 2.7530, Validation Accuracy: 40.98%\n",
      "Epoch 88/100, Training Loss: 1.6243, Training Accuracy: 57.06%\n",
      "Epoch 88/100, Validation Loss: 2.7420, Validation Accuracy: 42.06%\n",
      "Epoch 89/100, Training Loss: 1.7582, Training Accuracy: 55.20%\n",
      "Epoch 89/100, Validation Loss: 2.7448, Validation Accuracy: 40.69%\n",
      "Epoch 90/100, Training Loss: 1.6709, Training Accuracy: 57.06%\n",
      "Epoch 90/100, Validation Loss: 2.7347, Validation Accuracy: 40.00%\n",
      "Epoch 91/100, Training Loss: 1.6281, Training Accuracy: 57.84%\n",
      "Epoch 91/100, Validation Loss: 2.7840, Validation Accuracy: 40.69%\n",
      "Epoch 92/100, Training Loss: 1.6255, Training Accuracy: 58.33%\n",
      "Epoch 92/100, Validation Loss: 2.8036, Validation Accuracy: 41.76%\n",
      "Epoch 93/100, Training Loss: 1.5934, Training Accuracy: 59.41%\n",
      "Epoch 93/100, Validation Loss: 2.7369, Validation Accuracy: 41.37%\n",
      "Epoch 94/100, Training Loss: 1.5894, Training Accuracy: 58.92%\n",
      "Epoch 94/100, Validation Loss: 2.7451, Validation Accuracy: 41.67%\n",
      "Epoch 95/100, Training Loss: 1.6183, Training Accuracy: 58.53%\n",
      "Epoch 95/100, Validation Loss: 2.8204, Validation Accuracy: 41.27%\n",
      "Epoch 96/100, Training Loss: 1.6717, Training Accuracy: 57.84%\n",
      "Epoch 96/100, Validation Loss: 2.8316, Validation Accuracy: 41.27%\n",
      "Epoch 97/100, Training Loss: 1.7692, Training Accuracy: 54.51%\n",
      "Epoch 97/100, Validation Loss: 2.8008, Validation Accuracy: 39.61%\n",
      "Epoch 98/100, Training Loss: 1.7062, Training Accuracy: 57.45%\n",
      "Epoch 98/100, Validation Loss: 2.6874, Validation Accuracy: 40.59%\n",
      "Epoch 99/100, Training Loss: 1.7086, Training Accuracy: 56.96%\n",
      "Epoch 99/100, Validation Loss: 2.7313, Validation Accuracy: 42.94%\n",
      "Epoch 100/100, Training Loss: 1.5895, Training Accuracy: 58.24%\n",
      "Epoch 100/100, Validation Loss: 2.7369, Validation Accuracy: 42.25%\n"
     ]
    }
   ],
   "source": [
    "# Load the best trained model along with training and validation loss values\n",
    "# checkpoint = torch.load('best_trained_model.pth')\n",
    "# best_model_state = checkpoint['model_state_dict']\n",
    "# train_loss = checkpoint['train_loss']\n",
    "# val_loss = checkpoint['val_loss']\n",
    "# model.load_state_dict(best_model_state) # Set the model state and loss values before resuming training (Need to comment out the train_loss and val_loss)\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize the best validation loss with a large value (float('inf'))\n",
    "patience = 30  # Number of epochs to wait before stopping if validation loss doesn't improve\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Train the model\n",
    "    model.train() # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device) # Move images and labels to GPU\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        outputs = model.forward(images) # Forward pass\n",
    "        loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Optimize\n",
    "\n",
    "        train_loss += loss.item() * images.size(0) #  scalar value of the loss tensor for the current batch * the batch size to account for the loss per sample in the batch\n",
    "        _, predicted = outputs.max(1) # Returns a tuple containing the maximum value along the specified dimension (class probabilities for each sample in the batch) and index of the max value\n",
    "        total += labels.size(0) # Accumulates the total number of sample seen during training\n",
    "        correct += predicted.eq(labels).sum().item() # Accumulates the total number of correct predictions over all batches.\n",
    "    \n",
    "    # Calculate training loss and accuracy\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_accuracy = 100.0 * correct / total\n",
    "\n",
    "    # Print training loss and accuracy\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device) # Move images and labels to GPU\n",
    "            outputs = model.forward(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Calculate the loss\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss = val_loss / len(valid_loader.dataset)\n",
    "    val_accuracy = 100.0 * correct / total\n",
    "\n",
    "    # Print validation loss and accuracy\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"Creating new checkpoint for best model...\")\n",
    "        best_val_loss = val_loss\n",
    "        patience = 30  # Reset patience if validation loss improves\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "        current_val__loss = val_loss\n",
    "        current_train_loss = train_loss\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "# Save the best trained model along with training and validation loss values\n",
    "torch.save({\n",
    "    'model_state_dict': best_model_state,\n",
    "    'train_loss': current_train_loss,\n",
    "    'val_loss': current_val__loss\n",
    "}, 'best_trained_model.pth')\n",
    "\n",
    "# save the completed model training\n",
    "finish_model_state = model.state_dict()\n",
    "torch.save({\n",
    "    'model_state_dict': finish_model_state,\n",
    "    'train_loss': train_loss,\n",
    "    'val_loss': val_loss\n",
    "}, 'finish_trained_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
