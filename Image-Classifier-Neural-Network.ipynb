{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from PyTorch\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprosessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Randomly crop and resize the image\n",
    "    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(10),       # Randomly rotate the image by up to 10 degrees\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2),  # Randomly adjust brightness, contrast, and saturation\n",
    "    transforms.ToTensor(),               # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),              # Resize the image to 256x256\n",
    "    transforms.CenterCrop(224),          # Crop the center of the image to 224x224\n",
    "    transforms.ToTensor(),               # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Define dataset root directory\n",
    "data_dir = 'dataset_flower102/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the dataset during data loading\n",
    "train_dataset = datasets.Flowers102(root=data_dir, split='train', transform=train_transform, download=True)\n",
    "valid_dataset = datasets.Flowers102(root=data_dir, split='val', transform=val_transform, download=True)\n",
    "# test_dataset = datasets.Flowers102(root=data_dir, split='test', transform=data_transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for Neural Network\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Convolution Neural Network\n",
    "# Simple CNN architecture with two convolutional layers followed by max pooling, two fully connected layers, and a dropout layer for regularization.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max pooling layer: down-sample an image by applying max filer to subregion\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 512)  # Adjust input size based on image dimensions\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    \n",
    "    # Defines the forward pass of the network, where input data x is passed through each layer sequentially.\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activation and max pooling\n",
    "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
    "        # Flatten the output from convolutional layers\n",
    "        x = x.view(-1, 32 * 56 * 56)  # Adjust size based on image dimensions\n",
    "        # Fully connected layers with dropout\n",
    "        x = self.dropout(nn.ReLU()(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    # def __init__(self, num_classes):\n",
    "    #     super(CNN, self).__init__()\n",
    "    #     # Convolutional layers\n",
    "    #     self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "    #     self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "    #     self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "    #     # Max pooling layers\n",
    "    #     self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    #     # Fully connected layers\n",
    "    #     self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Adjust input size based on image dimensions\n",
    "    #     self.fc2 = nn.Linear(512, num_classes)\n",
    "    #     # Dropout layer to prevent overfitting\n",
    "    #     self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     # Convolutional layers with ReLU activation and max pooling\n",
    "    #     x = self.pool(nn.ReLU()(self.conv1(x)))\n",
    "    #     x = self.pool(nn.ReLU()(self.conv2(x)))\n",
    "    #     x = self.pool(nn.ReLU()(self.conv3(x)))\n",
    "    #     # Flatten the output from convolutional layers\n",
    "    #     x = x.view(-1, 128 * 28 * 28)  # Adjust size based on image dimensions\n",
    "    #     # Fully connected layers with dropout\n",
    "    #     x = self.dropout(nn.ReLU()(self.fc1(x)))\n",
    "    #     x = self.fc2(x)\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CNN model\n",
    "model = CNN(num_classes=102).to(device)  # 102 classes for Flowers102 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Training Loss: 1.6983, Training Accuracy: 49.90%\n",
      "Epoch 1/200, Validation Loss: 2.9001, Validation Accuracy: 43.53%\n",
      "Epoch 2/200, Training Loss: 1.6783, Training Accuracy: 51.96%\n",
      "Epoch 2/200, Validation Loss: 2.9554, Validation Accuracy: 42.06%\n",
      "Epoch 3/200, Training Loss: 1.6508, Training Accuracy: 55.00%\n",
      "Epoch 3/200, Validation Loss: 3.0144, Validation Accuracy: 41.96%\n",
      "Epoch 4/200, Training Loss: 1.6884, Training Accuracy: 52.06%\n",
      "Epoch 4/200, Validation Loss: 2.8546, Validation Accuracy: 42.75%\n",
      "Epoch 5/200, Training Loss: 1.6742, Training Accuracy: 53.82%\n",
      "Epoch 5/200, Validation Loss: 2.9239, Validation Accuracy: 42.25%\n",
      "Epoch 6/200, Training Loss: 1.8098, Training Accuracy: 50.29%\n",
      "Epoch 6/200, Validation Loss: 3.0392, Validation Accuracy: 41.67%\n",
      "Epoch 7/200, Training Loss: 1.7551, Training Accuracy: 52.35%\n",
      "Epoch 7/200, Validation Loss: 2.8046, Validation Accuracy: 41.27%\n",
      "Epoch 8/200, Training Loss: 1.7158, Training Accuracy: 51.37%\n",
      "Epoch 8/200, Validation Loss: 2.9825, Validation Accuracy: 41.96%\n",
      "Epoch 9/200, Training Loss: 1.7378, Training Accuracy: 51.08%\n",
      "Epoch 9/200, Validation Loss: 2.9268, Validation Accuracy: 42.25%\n",
      "Epoch 10/200, Training Loss: 1.7120, Training Accuracy: 50.49%\n",
      "Epoch 10/200, Validation Loss: 2.9698, Validation Accuracy: 42.06%\n",
      "Epoch 11/200, Training Loss: 1.7031, Training Accuracy: 52.45%\n",
      "Epoch 11/200, Validation Loss: 2.9276, Validation Accuracy: 42.75%\n",
      "Epoch 12/200, Training Loss: 1.6736, Training Accuracy: 54.41%\n",
      "Epoch 12/200, Validation Loss: 2.9719, Validation Accuracy: 43.33%\n",
      "Epoch 13/200, Training Loss: 1.6980, Training Accuracy: 51.67%\n",
      "Epoch 13/200, Validation Loss: 2.9315, Validation Accuracy: 43.14%\n",
      "Epoch 14/200, Training Loss: 1.7580, Training Accuracy: 50.29%\n",
      "Epoch 14/200, Validation Loss: 3.0187, Validation Accuracy: 40.39%\n",
      "Epoch 15/200, Training Loss: 1.7095, Training Accuracy: 52.06%\n",
      "Epoch 15/200, Validation Loss: 2.8895, Validation Accuracy: 41.47%\n",
      "Epoch 16/200, Training Loss: 1.7277, Training Accuracy: 50.00%\n",
      "Epoch 16/200, Validation Loss: 2.8273, Validation Accuracy: 40.59%\n",
      "Epoch 17/200, Training Loss: 1.7636, Training Accuracy: 51.18%\n",
      "Epoch 17/200, Validation Loss: 2.7627, Validation Accuracy: 45.98%\n",
      "Epoch 18/200, Training Loss: 1.7018, Training Accuracy: 51.47%\n",
      "Epoch 18/200, Validation Loss: 2.9075, Validation Accuracy: 42.06%\n",
      "Epoch 19/200, Training Loss: 1.7503, Training Accuracy: 50.98%\n",
      "Epoch 19/200, Validation Loss: 2.8386, Validation Accuracy: 42.25%\n",
      "Epoch 20/200, Training Loss: 1.6756, Training Accuracy: 55.20%\n",
      "Epoch 20/200, Validation Loss: 2.9038, Validation Accuracy: 42.55%\n",
      "Epoch 21/200, Training Loss: 1.7111, Training Accuracy: 52.06%\n",
      "Epoch 21/200, Validation Loss: 2.9517, Validation Accuracy: 44.12%\n",
      "Epoch 22/200, Training Loss: 1.6342, Training Accuracy: 53.82%\n",
      "Epoch 22/200, Validation Loss: 2.9445, Validation Accuracy: 41.96%\n",
      "Epoch 23/200, Training Loss: 1.6175, Training Accuracy: 53.82%\n",
      "Epoch 23/200, Validation Loss: 2.9447, Validation Accuracy: 41.57%\n",
      "Epoch 24/200, Training Loss: 1.7065, Training Accuracy: 54.02%\n",
      "Epoch 24/200, Validation Loss: 2.9164, Validation Accuracy: 42.94%\n",
      "Epoch 25/200, Training Loss: 1.6545, Training Accuracy: 52.45%\n",
      "Epoch 25/200, Validation Loss: 2.9961, Validation Accuracy: 42.45%\n",
      "Epoch 26/200, Training Loss: 1.5584, Training Accuracy: 56.27%\n",
      "Epoch 26/200, Validation Loss: 2.9361, Validation Accuracy: 43.14%\n",
      "Epoch 27/200, Training Loss: 1.6911, Training Accuracy: 52.45%\n",
      "Epoch 27/200, Validation Loss: 2.9397, Validation Accuracy: 42.75%\n",
      "Epoch 28/200, Training Loss: 1.6986, Training Accuracy: 55.10%\n",
      "Epoch 28/200, Validation Loss: 2.9339, Validation Accuracy: 42.84%\n",
      "Epoch 29/200, Training Loss: 1.6541, Training Accuracy: 54.90%\n",
      "Epoch 29/200, Validation Loss: 2.9014, Validation Accuracy: 44.31%\n",
      "Epoch 30/200, Training Loss: 1.6738, Training Accuracy: 54.31%\n",
      "Epoch 30/200, Validation Loss: 2.9611, Validation Accuracy: 43.53%\n",
      "Epoch 31/200, Training Loss: 1.5925, Training Accuracy: 56.18%\n",
      "Epoch 31/200, Validation Loss: 2.8669, Validation Accuracy: 43.33%\n",
      "Epoch 32/200, Training Loss: 1.6867, Training Accuracy: 51.96%\n",
      "Epoch 32/200, Validation Loss: 2.9140, Validation Accuracy: 42.94%\n",
      "Epoch 33/200, Training Loss: 1.6029, Training Accuracy: 55.98%\n",
      "Epoch 33/200, Validation Loss: 2.9710, Validation Accuracy: 42.75%\n",
      "Epoch 34/200, Training Loss: 1.5534, Training Accuracy: 55.88%\n",
      "Epoch 34/200, Validation Loss: 3.0036, Validation Accuracy: 42.75%\n",
      "Epoch 35/200, Training Loss: 1.6780, Training Accuracy: 53.82%\n",
      "Epoch 35/200, Validation Loss: 2.8941, Validation Accuracy: 42.55%\n",
      "Epoch 36/200, Training Loss: 1.6846, Training Accuracy: 54.80%\n",
      "Epoch 36/200, Validation Loss: 2.9157, Validation Accuracy: 43.73%\n",
      "Epoch 37/200, Training Loss: 1.5337, Training Accuracy: 57.84%\n",
      "Epoch 37/200, Validation Loss: 3.1072, Validation Accuracy: 42.75%\n",
      "Epoch 38/200, Training Loss: 1.6692, Training Accuracy: 54.31%\n",
      "Epoch 38/200, Validation Loss: 2.9858, Validation Accuracy: 42.45%\n",
      "Epoch 39/200, Training Loss: 1.6612, Training Accuracy: 56.27%\n",
      "Epoch 39/200, Validation Loss: 2.7978, Validation Accuracy: 45.29%\n",
      "Epoch 40/200, Training Loss: 1.6619, Training Accuracy: 55.00%\n",
      "Epoch 40/200, Validation Loss: 2.8798, Validation Accuracy: 43.82%\n",
      "Epoch 41/200, Training Loss: 1.7448, Training Accuracy: 51.27%\n",
      "Epoch 41/200, Validation Loss: 2.9394, Validation Accuracy: 41.96%\n",
      "Epoch 42/200, Training Loss: 1.6579, Training Accuracy: 53.92%\n",
      "Epoch 42/200, Validation Loss: 2.8933, Validation Accuracy: 44.22%\n",
      "Epoch 43/200, Training Loss: 1.6209, Training Accuracy: 55.29%\n",
      "Epoch 43/200, Validation Loss: 2.9247, Validation Accuracy: 43.33%\n",
      "Epoch 44/200, Training Loss: 1.6210, Training Accuracy: 53.73%\n",
      "Epoch 44/200, Validation Loss: 2.8392, Validation Accuracy: 44.51%\n",
      "Epoch 45/200, Training Loss: 1.4581, Training Accuracy: 57.75%\n",
      "Epoch 45/200, Validation Loss: 2.9315, Validation Accuracy: 43.53%\n",
      "Epoch 46/200, Training Loss: 1.6376, Training Accuracy: 53.63%\n",
      "Epoch 46/200, Validation Loss: 3.0784, Validation Accuracy: 43.24%\n",
      "Epoch 47/200, Training Loss: 1.6162, Training Accuracy: 55.20%\n",
      "Epoch 47/200, Validation Loss: 2.9937, Validation Accuracy: 43.43%\n",
      "Epoch 48/200, Training Loss: 1.6541, Training Accuracy: 55.59%\n",
      "Epoch 48/200, Validation Loss: 3.0301, Validation Accuracy: 42.94%\n",
      "Epoch 49/200, Training Loss: 1.6435, Training Accuracy: 53.14%\n",
      "Epoch 49/200, Validation Loss: 2.9354, Validation Accuracy: 42.84%\n",
      "Epoch 50/200, Training Loss: 1.5970, Training Accuracy: 55.39%\n",
      "Epoch 50/200, Validation Loss: 2.9064, Validation Accuracy: 43.24%\n",
      "Epoch 51/200, Training Loss: 1.7252, Training Accuracy: 52.35%\n",
      "Epoch 51/200, Validation Loss: 2.8334, Validation Accuracy: 42.45%\n",
      "Epoch 52/200, Training Loss: 1.6134, Training Accuracy: 54.51%\n",
      "Epoch 52/200, Validation Loss: 2.9571, Validation Accuracy: 44.80%\n",
      "Epoch 53/200, Training Loss: 1.6571, Training Accuracy: 55.29%\n",
      "Epoch 53/200, Validation Loss: 3.0836, Validation Accuracy: 42.84%\n",
      "Epoch 54/200, Training Loss: 1.6983, Training Accuracy: 52.45%\n",
      "Epoch 54/200, Validation Loss: 2.9581, Validation Accuracy: 44.41%\n",
      "Epoch 55/200, Training Loss: 1.5764, Training Accuracy: 57.55%\n",
      "Epoch 55/200, Validation Loss: 2.9419, Validation Accuracy: 42.94%\n",
      "Epoch 56/200, Training Loss: 1.6197, Training Accuracy: 55.29%\n",
      "Epoch 56/200, Validation Loss: 2.9819, Validation Accuracy: 43.53%\n",
      "Epoch 57/200, Training Loss: 1.5254, Training Accuracy: 56.47%\n",
      "Epoch 57/200, Validation Loss: 2.9303, Validation Accuracy: 42.84%\n",
      "Epoch 58/200, Training Loss: 1.5476, Training Accuracy: 56.86%\n",
      "Epoch 58/200, Validation Loss: 3.0878, Validation Accuracy: 41.76%\n",
      "Epoch 59/200, Training Loss: 1.5258, Training Accuracy: 56.08%\n",
      "Epoch 59/200, Validation Loss: 2.9461, Validation Accuracy: 43.43%\n",
      "Epoch 60/200, Training Loss: 1.6063, Training Accuracy: 55.10%\n",
      "Epoch 60/200, Validation Loss: 3.1096, Validation Accuracy: 44.80%\n",
      "Epoch 61/200, Training Loss: 1.5569, Training Accuracy: 55.20%\n",
      "Epoch 61/200, Validation Loss: 2.9530, Validation Accuracy: 44.90%\n",
      "Epoch 62/200, Training Loss: 1.6052, Training Accuracy: 56.37%\n",
      "Epoch 62/200, Validation Loss: 3.0735, Validation Accuracy: 43.73%\n",
      "Epoch 63/200, Training Loss: 1.6184, Training Accuracy: 53.82%\n",
      "Epoch 63/200, Validation Loss: 2.9732, Validation Accuracy: 43.82%\n",
      "Epoch 64/200, Training Loss: 1.4805, Training Accuracy: 59.22%\n",
      "Epoch 64/200, Validation Loss: 3.0288, Validation Accuracy: 43.24%\n",
      "Epoch 65/200, Training Loss: 1.6009, Training Accuracy: 55.49%\n",
      "Epoch 65/200, Validation Loss: 2.9373, Validation Accuracy: 42.75%\n",
      "Epoch 66/200, Training Loss: 1.5138, Training Accuracy: 56.08%\n",
      "Epoch 66/200, Validation Loss: 3.0277, Validation Accuracy: 44.22%\n",
      "Epoch 67/200, Training Loss: 1.5845, Training Accuracy: 54.41%\n",
      "Epoch 67/200, Validation Loss: 2.9472, Validation Accuracy: 44.61%\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "# Load the best trained model along with training and validation loss values\n",
    "checkpoint = torch.load('best_trained_model.pth')\n",
    "best_model_state = checkpoint['model_state_dict']\n",
    "train_loss = checkpoint['train_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "\n",
    "# Set the model state and loss values before resuming training (Need to comment out the train_loss and val_loss)\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize the best validation loss with a large value\n",
    "patience = 50  # Number of epochs to wait before stopping if validation loss doesn't improve\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Train the model\n",
    "    model.train() # Set the model to training mode\n",
    "    # train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device) # Move images and labels to GPU\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        outputs = model.forward(images) # Forward pass\n",
    "        loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Optimize\n",
    "\n",
    "        train_loss += loss.item() * images.size(0) #  scalar value of the loss tensor for the current batch * the batch size to account for the loss per sample in the batch\n",
    "        _, predicted = outputs.max(1) # Returns a tuple containing the maximum value along the specified dimension (class probabilities for each sample in the batch) and index of the max value\n",
    "        total += labels.size(0) # Accumulates the total number of sample seen during training\n",
    "        correct += predicted.eq(labels).sum().item() # Accumulates the total number of correct predictions over all batches.\n",
    "    \n",
    "    # Calculate training loss and accuracy\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_accuracy = 100.0 * correct / total\n",
    "\n",
    "    # Print training loss and accuracy\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    # val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device) # Move images and labels to GPU\n",
    "            outputs = model.forward(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Calculate the loss\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss = val_loss / len(valid_loader.dataset)\n",
    "    val_accuracy = 100.0 * correct / total\n",
    "\n",
    "    # Print validation loss and accuracy\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"Creating new checkpoint for best model...\")\n",
    "        best_val_loss = val_loss\n",
    "        patience = 50  # Reset patience if validation loss improves\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "    # else:\n",
    "    #     patience -= 1\n",
    "    #     if patience == 0:\n",
    "    #         print(\"Early stopping...\")\n",
    "    #         break\n",
    "\n",
    "# Save the best trained model along with training and validation loss values\n",
    "torch.save({\n",
    "    'model_state_dict': best_model_state,\n",
    "    'train_loss': train_loss,\n",
    "    'val_loss': val_loss\n",
    "}, 'best_trained_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
